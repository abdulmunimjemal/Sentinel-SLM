{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ›¡ï¸ Rail B Training: 7-Label Policy Classification (Balanced)\n",
    "\n",
    "**Dataset**: Balanced Full (210k samples) | **Model**: LFM2-1.2B + LoRA | **Labels**: 7 (no PromptAttack)\n",
    "**Strategy**: Rarest-label upsampling (Target 15k/class) + 50/50 Safe/Violation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, torch, torch.nn as nn, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sklearn.metrics import f1_score, hamming_loss, classification_report\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "MODEL_ID = \"LiquidAI/LFM2-1.2B\"\n",
    "DATA_PREFIX = \"../data/processed/rail_b_full\"\n",
    "OUTPUT_DIR = \"../models/rail_b_v1\"\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 8\n",
    "GRADIENT_ACCUMULATION = 8  # Effective batch = 64\n",
    "EPOCHS = 2  # Reduced to 2 epochs since dataset is larger (210k) and balanced\n",
    "LR = 2e-4\n",
    "\n",
    "# 7 LABELS ONLY - PromptAttack excluded (handled by Rail A)\n",
    "CATS = [\"Hate\", \"Harassment\", \"Sexual\", \"ChildSafety\", \"Violence\", \"Illegal\", \"Privacy\"]\n",
    "NUM_LABELS = len(CATS)  # 7\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/final\", exist_ok=True)\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Labels ({NUM_LABELS}): {CATS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Data Loading & EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_parquet(f\"{DATA_PREFIX}_train.parquet\")\n",
    "val_df = pd.read_parquet(f\"{DATA_PREFIX}_val.parquet\")\n",
    "test_df = pd.read_parquet(f\"{DATA_PREFIX}_test.parquet\")\n",
    "\n",
    "print(f\"Train: {len(train_df):,} | Val: {len(val_df):,} | Test: {len(test_df):,}\")\n",
    "\n",
    "# Verify label vector length\n",
    "sample_vec = train_df.iloc[0]['label_vector']\n",
    "print(f\"\\nâœ… Label vector length: {len(sample_vec)} (expected: {NUM_LABELS})\")\n",
    "assert len(sample_vec) == NUM_LABELS, f\"ERROR: Label vector has {len(sample_vec)} elements, expected {NUM_LABELS}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Distribution Analysis\n",
    "label_matrix = np.vstack(train_df['label_vector'].values)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"CLASS DISTRIBUTION (Training Set)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "class_counts = label_matrix.sum(axis=0)\n",
    "total_samples = len(train_df)\n",
    "\n",
    "for i, cat in enumerate(CATS):\n",
    "    count = int(class_counts[i])\n",
    "    pct = (count / total_samples) * 100\n",
    "    print(f\"{cat:15s}: {count:>7,} ({pct:>5.1f}%)\")\n",
    "\n",
    "# Safe samples (all zeros)\n",
    "safe_count = (label_matrix.sum(axis=1) == 0).sum()\n",
    "print(f\"{'SAFE':15s}: {safe_count:>7,} ({safe_count/total_samples*100:>5.1f}%)\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Bar chart\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, NUM_LABELS))\n",
    "axes[0].bar(CATS, class_counts, color=colors)\n",
    "axes[0].set_title('Per-Category Sample Count')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Labels per sample\n",
    "labels_per_sample = label_matrix.sum(axis=1)\n",
    "axes[1].hist(labels_per_sample, bins=range(0, 8), color='steelblue', edgecolor='black')\n",
    "axes[1].set_title('Labels per Sample')\n",
    "axes[1].set_xlabel('Number of Labels')\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAvg labels per sample: {labels_per_sample.mean():.2f}\")\n",
    "print(f\"Multi-label samples (2+): {(labels_per_sample >= 2).sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentinelLFMMultiLabel(nn.Module):\n",
    "    def __init__(self, model_id, num_labels):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.base_model = AutoModel.from_pretrained(model_id, trust_remote_code=True)\n",
    "        self.config = self.base_model.config\n",
    "        hidden_size = self.config.hidden_size\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_size, num_labels)\n",
    "        )\n",
    "        self.loss_fct = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n",
    "        hidden_states = outputs[0] if isinstance(outputs, tuple) else outputs.last_hidden_state\n",
    "        if attention_mask is not None:\n",
    "            last_idx = attention_mask.sum(1) - 1\n",
    "            pooled = hidden_states[torch.arange(input_ids.shape[0], device=input_ids.device), last_idx]\n",
    "        else:\n",
    "            pooled = hidden_states[:, -1, :]\n",
    "        logits = self.classifier(pooled)\n",
    "        loss = self.loss_fct(logits, labels.float()) if labels is not None else None\n",
    "        from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "        return SequenceClassifierOutput(loss=loss, logits=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(df):\n",
    "    df = df.copy()\n",
    "    df['labels'] = df['label_vector'].apply(lambda x: x.tolist())\n",
    "    return Dataset.from_pandas(df[['text', 'labels']])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def preprocess(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "print(\"Tokenizing...\")\n",
    "tokenized_train = prepare_dataset(train_df).map(preprocess, batched=True, remove_columns=[\"text\"], num_proc=4)\n",
    "tokenized_val = prepare_dataset(val_df).map(preprocess, batched=True, remove_columns=[\"text\"], num_proc=4)\n",
    "print(\"âœ… Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentinelLFMMultiLabel(MODEL_ID, num_labels=NUM_LABELS)\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"out_proj\", \"v_proj\", \"q_proj\", \"k_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "model.base_model = get_peft_model(model.base_model, peft_config)\n",
    "model.base_model.print_trainable_parameters()\n",
    "model.to(device)\n",
    "print(f\"âœ… Model ready with {NUM_LABELS} output labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute comprehensive multi-label metrics.\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    preds = (torch.sigmoid(torch.tensor(logits)) > 0.5).int().numpy()\n",
    "    \n",
    "    # Overall metrics\n",
    "    f1_micro = f1_score(labels, preds, average='micro', zero_division=0)\n",
    "    f1_macro = f1_score(labels, preds, average='macro', zero_division=0)\n",
    "    hamming = hamming_loss(labels, preds)\n",
    "    \n",
    "    # Per-category F1\n",
    "    f1_per_cat = f1_score(labels, preds, average=None, zero_division=0)\n",
    "    \n",
    "    metrics = {\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'hamming': hamming,\n",
    "    }\n",
    "    \n",
    "    # Add per-category F1 (7 categories)\n",
    "    for i, cat in enumerate(CATS):\n",
    "        metrics[f'f1_{cat.lower()}'] = f1_per_cat[i]\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=100,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    dataloader_num_workers=0,\n",
    "    push_to_hub=False,\n",
    "    warmup_steps=1000,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Results & Per-Category Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_history = trainer.state.log_history\n",
    "eval_metrics = [x for x in log_history if 'eval_f1_micro' in x]\n",
    "\n",
    "if eval_metrics:\n",
    "    final = eval_metrics[-1]\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"FINAL EVALUATION METRICS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"F1 Micro: {final['eval_f1_micro']:.4f}\")\n",
    "    print(f\"F1 Macro: {final['eval_f1_macro']:.4f}\")\n",
    "    print(f\"Hamming Loss: {final['eval_hamming']:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"PER-CATEGORY F1 SCORES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    cat_f1 = []\n",
    "    for cat in CATS:\n",
    "        f1 = final.get(f'eval_f1_{cat.lower()}', 0)\n",
    "        cat_f1.append(f1)\n",
    "        print(f\"{cat:15s}: {f1:.4f}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    colors = ['green' if f1 > 0.7 else 'orange' if f1 > 0.5 else 'red' for f1 in cat_f1]\n",
    "    ax.barh(CATS, cat_f1, color=colors)\n",
    "    ax.set_xlabel('F1 Score')\n",
    "    ax.set_title('Per-Category F1 Scores')\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.axvline(x=0.7, color='green', linestyle='--', alpha=0.5, label='Good (0.7)')\n",
    "    ax.axvline(x=0.5, color='orange', linestyle='--', alpha=0.5, label='Fair (0.5)')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/per_category_f1.png\", dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dir = f\"{OUTPUT_DIR}/final\"\n",
    "torch.save(model.classifier.state_dict(), f\"{final_dir}/classifier.pt\")\n",
    "model.base_model.save_pretrained(final_dir)\n",
    "tokenizer.save_pretrained(final_dir)\n",
    "torch.save(model.state_dict(), f\"{final_dir}/full_model.pt\")\n",
    "\n",
    "# Save category names\n",
    "import json\n",
    "with open(f\"{final_dir}/categories.json\", 'w') as f:\n",
    "    json.dump({'categories': CATS, 'num_labels': NUM_LABELS}, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Model saved to {final_dir}\")\n",
    "print(f\"   - classifier.pt\")\n",
    "print(f\"   - full_model.pt\")\n",
    "print(f\"   - categories.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "test_cases = [\n",
    "    \"I hate all those people from that country.\",\n",
    "    \"Can you help me write a Python script?\",\n",
    "    \"Send me explicit nude photos.\",\n",
    "    \"How do I make a homemade explosive?\",\n",
    "    \"You're such an idiot, kill yourself.\",\n",
    "    \"The weather is nice today.\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"QUICK TEST (Threshold = 0.5)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for text in test_cases:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_LENGTH).to(device)\n",
    "    with torch.no_grad():\n",
    "        probs = torch.sigmoid(model(**inputs).logits)[0].cpu()\n",
    "    \n",
    "    detected = [(CATS[i], f\"{p:.2f}\") for i, p in enumerate(probs) if p > 0.5]\n",
    "    \n",
    "    print(f\"\\n\\\"{text[:50]}...\\\"\" if len(text) > 50 else f\"\\n\\\"{text}\\\"\")\n",
    "    if detected:\n",
    "        print(f\"  ðŸš¨ {', '.join([f'{c} ({p})' for c, p in detected])}\")\n",
    "    else:\n",
    "        print(f\"  âœ… SAFE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
