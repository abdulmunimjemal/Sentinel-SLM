{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Sentinel-SLM Data Preparation\n",
    "\n",
    "This notebook orchestrates the data preparation pipeline by importing logic from the `src.sentinel.data` package.\n",
    "You can run this step-by-step to verify downloads and mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure repo root is in path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from src.sentinel.data.download import download_all\n",
    "from src.sentinel.data.processing import map_all_raw_data\n",
    "from src.sentinel.utils.taxonomy import CATEGORY_NAMES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. Download Public Datasets\n",
    "This step fetches data from Hugging Face and saves it to `data/raw/*.parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Downloading All Data ---\")\n",
    "download_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2. Inspect Raw Data\n",
    "Let's check one of the downloaded files to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path = \"../data/raw/civil_comments_sample.parquet\"\n",
    "if os.path.exists(raw_path):\n",
    "    df_raw = pd.read_parquet(raw_path)\n",
    "    print(f\"Loaded {raw_path} with {len(df_raw)} rows.\")\n",
    "    display(df_raw.head())\n",
    "else:\n",
    "    print(f\"{raw_path} not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 3. Map & Standardize Labels\n",
    "Run the mapping logic to convert all raw datasets into our 8-category taxonomy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This runs the main processing pipeline\n",
    "map_all_raw_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 4. Final Distribution Audit\n",
    "Analyze the class abundance in the unified dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_path = \"../data/processed/unified_dataset.parquet\"\n",
    "if os.path.exists(processed_path):\n",
    "    df_uni = pd.read_parquet(processed_path)\n",
    "    print(f\"Total Processed Samples: {len(df_uni)}\")\n",
    "    \n",
    "    # Explode because labels are lists\n",
    "    exploded = df_uni.explode(\"labels\")\n",
    "    counts = exploded[\"labels\"].value_counts().rename(index=CATEGORY_NAMES)\n",
    "    \n",
    "    print(\"\\n--- Class Balance ---\")\n",
    "    print(counts)\n",
    "    \n",
    "    # Visual check of some mapped examples\n",
    "    print(\"\\n--- Random Examples ---\")\n",
    "    sample = df_uni.sample(5)\n",
    "    for i, row in sample.iterrows():\n",
    "        cats = [CATEGORY_NAMES.get(l, l) for l in row['labels']]\n",
    "        print(f\"Text (trunc): {row['text'][:100]}...\\nLabels: {cats}\\n\")\n",
    "else:\n",
    "    print(\"Unified dataset not found. Did mapping fail?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
